{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from config import *\n",
    "from imdb import kitti\n",
    "from utils.util import *\n",
    "from nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('dataset','KITTI',\"\"\"Current kitti dataset.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_path','',\"\"\"Root directory of data\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_set','train',\"\"\"can be set as train, trainval,val or test\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir','../log',\"\"\"event log\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps',1000000,\"\"\"maximum number of batches to run\"\"\")\n",
    "tf.app.flags.DEFINE_string('net','squeezeSeg',\"\"\"net architecture\"\"\")\n",
    "tf.app.flags.DEFINE_string('pretrained_model_path','',\"\"\"path to pretrained model\"\"\")\n",
    "tf.app.flags.DEFINE_integer('summary_step',10,\"\"\"Number of steps to save summary\"\"\")\n",
    "tf.app.flags.DEFINE_integer('checkpoint_step',5000,\"\"\"number of steps to save\"\"\")\n",
    "tf.app.flags.DEFINE_string('gpu','0',\"\"\"gpu_id\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    assert FLAGS.dataset =='KITTI','Currently only support KITTI dataset'\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        assert FLAGS.net =='squeezeSeg','Selected neural net architecture not supported:{}'.format(FLAGS.net) \n",
    "        \n",
    "        if FLAGS.net == 'squeezeSeg':\n",
    "            mc = kitti_squeezeSeg_config()\n",
    "            mc.PRETRAINED_MODEL_PATH = FLAGS.pretrained_model_path\n",
    "            model = SqueezeSeg(mc)\n",
    "        imdb = kitti(FLAGS.image_set,FLAGS.data_path,mc)\n",
    "        \n",
    "        with open(os.path.join(FLAGS.train_dir, 'model_metrics.txt'), 'w') as f:\n",
    "            f.write('Number of parameter by layer:\\n')\n",
    "            count = 0\n",
    "            for c in model.model_size_counter:\n",
    "                f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "                count += c[1]\n",
    "            f.write('\\ttotal: {}\\n'.format(count))\n",
    "\n",
    "            count = 0\n",
    "            f.write('\\nActivation size by layer:\\n')\n",
    "            for c in model.activation_counter:\n",
    "                f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "                count += c[1]\n",
    "            f.write('\\ttotal: {}\\n'.format(count))\n",
    "\n",
    "            count = 0\n",
    "            f.write('\\nNumber of flops by layer:\\n')\n",
    "            for c in model.flop_counter:\n",
    "                f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "                count += c[1]\n",
    "            f.write('\\ttotal: {}\\n'.format(count))\n",
    "        f.close()\n",
    "        print ('Model statistics saved to {}.'.format(os.path.join(FLAGS.train_dir, 'model_metrics.txt')))\n",
    "        \n",
    "        def enqueue(sess,coord):\n",
    "            with coord.stop_on_exception():\n",
    "                while not coord.should_stop():\n",
    "                    #read batch input\n",
    "                    lidar_per_batch,lidar_mask_per_batch,label_per_batch,\\\n",
    "                    weight_per_batch = imdb.read_batch()\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        model.ph_keep_prob: mc.KEEP_PROB,\n",
    "                        model.ph.lidar_input: lidar_per_batch,\n",
    "                        model.ph_lidar_mask:  lidar_mask_per_batch,\n",
    "                        model.ph_label: label_per_batch,\n",
    "                        model.ph_loss_weight:weight_per_batch,\n",
    "                        \n",
    "                        \n",
    "                    }\n",
    "                    \n",
    "                    sess.run(model.enqueue_op,feed_dict=feed_dict)\n",
    "                saver = tf.train.Saver(tf.all_variables())\n",
    "                summary_op = tf.summary.merge_all()\n",
    "                \n",
    "                init = tf.initialize_all_variables()\n",
    "                \n",
    "                sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "\n",
    "                sess.run(init)\n",
    "                summary_writer = tf.summary.FileWriter(FLAGS.train_dir,sess.graph)\n",
    "                \n",
    "                coord = tf.train.Coordinator()\n",
    "                enq_threads = []\n",
    "                for _ in range (mc.NUM_ENQUEUE_THREAD):\n",
    "                    eqth = threading.Thread(target=enqueue,args = [sess,coord])\n",
    "                    eqth.start()\n",
    "                    enq_threads.append(eqth)\n",
    "                    \n",
    "                run_options = tf.RunOptions(timeout_in_ms=60000)\n",
    "                \n",
    "                try:\n",
    "                    for step in xrange(FLAGS.max_steps):\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        if step % FLAGS.summary_step == 0 or step == FLAGS.max_steps-1:\n",
    "                            op_list = [\n",
    "                                model.lidar_input, model.lidar_mask_per_batch,model.label,model.train_op,\n",
    "                                model.loss,model.pred_cls,summary_op\n",
    "                            ]\n",
    "                            lidar_per_batch,lidar_mask_per_batch,label_per_batch,_,loss_value,\\\n",
    "                            pred_cls,summary_str =sess.run(op_list,option=run_options)\n",
    "                            \n",
    "                            label_image = visualize_seg(label_per_batch[:6,:,:],mc)\n",
    "                            pred_image = visualize_seg(pred_cls[:6,:,:],mc)\n",
    "                            \n",
    "                            ious, _, _, _ = evaluate_iou(\n",
    "                            label_per_batch, pred_cls*np.squeeze(lidar_mask_per_batch),mc.NUM_CLASS)\n",
    "\n",
    "                            feed_dict = {}\n",
    "                            # Assume that class-0 is the background class\n",
    "                            for i in range(1, mc.NUM_CLASS):\n",
    "                                feed_dict[model.iou_summary_placeholders[i]] = ious[i]\n",
    "\n",
    "                                iou_summary_list = sess.run(model.iou_summary_ops[1:], feed_dict)\n",
    "\n",
    "                                # Run visualization\n",
    "                                viz_op_list = [model.show_label, model.show_depth_img, model.show_pred]\n",
    "                                viz_summary_list = sess.run(\n",
    "                                                  viz_op_list, \n",
    "                                                  feed_dict={\n",
    "                  model.depth_image_to_show: lidar_per_batch[:6, :, :, [4]],\n",
    "                  model.label_to_show: label_image,\n",
    "                  model.pred_image_to_show: pred_image,\n",
    "              }\n",
    "          )\n",
    " \n",
    "        \n",
    "                            summary_writer.add_summary(summary,step)\n",
    "                            for sum_str in iou_summary_list:\n",
    "                                summary_writer.add_summary(sum_str,step)\n",
    "                    \n",
    "                            for viz_sum in viz_summary_list:\n",
    "                                summary_writer.add_summary(viz_sum,step)\n",
    "                                \n",
    "                            summary_writer.flush()\n",
    "                                \n",
    "                        else:\n",
    "                            _,loss_value = sess.run([model.train_op,model.loss],option=run_options)\n",
    "                            \n",
    "                        duration = time.time()-start_time\n",
    "                        \n",
    "                        \n",
    "                        if step %10 ==0:\n",
    "                            num_images_per_step = mc.BATCH_SIZE\n",
    "                            images_per_sec = num_images_per_step / duration\n",
    "                            sec_per_batch = float(duration)\n",
    "                            format_str =('%s: step %d, loss= %.2f (%.1f images/sec; %.3f sec/batch)')\n",
    "                            \n",
    "                            print(format_str % (datetime.now(),step,loss_value,images_per_sec,sec_per_batch))\n",
    "                            \n",
    "                            sys.stdout.flush()\n",
    "                            \n",
    "                        #Save checkpoint \n",
    "                        \n",
    "                        if step % FLAGS.checkpoint_step == 0 or step == FLAGS.max_steps-1:\n",
    "                            checkpoint_path = os.path.join(FLAGS.train_dir,'model.ckpt')\n",
    "                            saver.save(sess,checkpoint_path,global_step=step)\n",
    "                        \n",
    "                except Exception,e:\n",
    "                        coord.request_stop(e)\n",
    "                finally:\n",
    "                        coord.request_stop()\n",
    "                        sess.run(model.q.close(cancel_pending_enqueues=True))\n",
    "                        coord.join(enq_threads)\n",
    "                        \n",
    "        \n",
    "                            \n",
    "                            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cannot find pretrained model at the given path:  ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8136abc271d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8136abc271d1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-55b92ed10edc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkitti_squeezeSeg_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_MODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSqueezeSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkitti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/app/wangyuan/wy/code/dense_fcn_with_squeezeSeg/src/nets/squeezeSeg.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mc, gpu_id)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mModelSkeleton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_forward_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_output_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_loss_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/app/wangyuan/wy/code/dense_fcn_with_squeezeSeg/src/nets/squeezeSeg.pyc\u001b[0m in \u001b[0;36m_add_forward_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0;34m'Cannot find pretrained model at the given path:'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0;34m'  {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaffemodel_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot find pretrained model at the given path:  "
     ]
    }
   ],
   "source": [
    "def main(argv = None):\n",
    "    if tf.gfile.Exists(FLAGS.train_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "    train()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
