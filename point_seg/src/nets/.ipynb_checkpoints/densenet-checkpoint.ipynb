{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import batch_norm,flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3DLayer(input_layer,input_dim,output_dim,height,width,length,stride,activation=tf.nn.relu,padding='SAME',\n",
    "               name='',is_training=True):\n",
    "    with tf.variable_scope(\"conv3D\" + name):\n",
    "        kernel = tf.get_variable(\"weights\",shape=[length,height,width,input_dim,output_dim],\\\n",
    "                                dtype=tf.float32,initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable('bias',shape=[output_dim],dtype=tf.float32,initializer=tf.constant_initializert(0.0))\n",
    "        conv = tf.nn.conv3d(input_layer,kernel,stride,padding=padding)\n",
    "        bias = tf.nn.bias_add(conv,b)\n",
    "        if activation:\n",
    "            print('avtivation')\n",
    "            bias = activation(bias,name='activation')\n",
    "    return bias11\n",
    "\n",
    "def con3D_to_output(input_layet,input_dim,output_dim,height,width,length,stride, activation= tf.nn.relu,padding='SAME',\n",
    "                   name=''):\n",
    "    with tf.variable_scope('conv3D_out'+name):\n",
    "        kernel = tf.get_variable('weights',shape=[length,height,width,input_dim,output_dim],dtype=tf.float32,initializer=tf.constant_initializer(0.01))\n",
    "        conv = tf.nn.conv3d(input_layer,kernel,strides,padding=padding)\n",
    "    return conv\n",
    "\n",
    "def deconv3D_to_output(input_layer,input_dim,output_dim,height,width,length,stride,\n",
    "                      output_shape,activation=tf.nn.relu,padding='SAME',name=''):\n",
    "    with tf.variable_scope('deconv3D'+ name):\n",
    "        kernel = tf.get_variable('weights',shape=[length,height,width,output_dim,input_dim],dtype =tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.01))\n",
    "        deconv = tf.nn.conv3d_transpose(input_layer,kernel,output_shape,stride,padding='SAME')\n",
    "    return deconv\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(input_layer,shape,name='',is_training=True):\n",
    "    with tf.variable_scope('fully'+name):\n",
    "        kernel = tf.get_variable('weights',shape=shape,dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        fully = tf.matmul(input_layer,kernel)\n",
    "        fully = tf.nn.relu(fully)\n",
    "        fully =batch_norm(fully,is_training)\n",
    "        return fully\n",
    "\n",
    "def max_pool(input_layer,ksizes,stride,name ='',padding='SAME'):\n",
    "    with tf.variable_scope(\"Max_pool3d\"+ name):\n",
    "        pool = tf.nn.max_pool3d(input=input_layer,ksize=ksizes,strides=stride,padding=padding,name=name)\n",
    "    return pool\n",
    "def average_poo(input_layer,ksizes,stride,name='',padding='SAME'):\n",
    "    with tf.variable_scope('average3d'+name):\n",
    "        avg = tf.nn.avg_pool3d(input=input_layer,ksize=ksizes,strides=stride,padding=padding,name=name)\n",
    "    return avg\n",
    "\n",
    "def conv_layer(input,filter,stride=1,layer_name='CONV'):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        network = tf.layers.conv3d(inputs=input,use_bias=False,filters=filter,kernel_size=kernel,strides=stride,padding='SAME',\n",
    "                                  name=layer_name+'c')\n",
    "        \n",
    "        return network\n",
    "    \n",
    "def Drop_out(input_layer,rate,training):\n",
    "    return tf.layers.dropout(inputs=input_layer,rate=rate,training=training)\n",
    "\n",
    "def Concatenation(layers):\n",
    "    return tf.concat(layers,axis=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=False),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "    \n",
    "def Batch_norm(inputs, phase_train, decay=0.9, eps=1e-5,name=''):\n",
    "    \"\"\"Batch Normalization\n",
    "\n",
    "       Args:\n",
    "           inputs: input data(Batch size) from last layer\n",
    "           phase_train: when you test, please set phase_train \"None\"\n",
    "       Returns:\n",
    "           output for next layer\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('BN'+name):\n",
    "        gamma = tf.get_variable(\"gamma\", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n",
    "        beta = tf.get_variable(\"beta\", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "        pop_mean = tf.get_variable(\"pop_mean\", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "        pop_var = tf.get_variable(\"pop_var\", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n",
    "        axes = range(len(inputs.get_shape()) - 1)\n",
    "\n",
    "        if phase_train != None:\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs, axes)\n",
    "            train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean*(1 - decay))\n",
    "            train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, eps)\n",
    "        else:\n",
    "            return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_layer():\n",
    "    def __init__(self,nb_blocks,filters,training):\n",
    "        self.nb_blocks = nb_blocks\n",
    "        self.filters   = filters\n",
    "        self.training  = training\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
